# ðŸ›¡ï¸ AI-Security Prompt Filter with n8n

A no-code/low-code prompt security pipeline built using **n8n**, designed to classify and log AI prompts using:
- âœ… Regex-based threat pattern matching
- ðŸ¤– LLM-assisted classification via **Gemini API**
- ðŸ“ Google Sheets integration for real-time logging, review, and updating

---

## ðŸ“Œ Project Highlights

| Feature                           | Description |
|----------------------------------|-------------|
| ðŸ” Regex Filtering                | Pattern-matches user prompts against known threats using dynamic regex rules from Google Sheets |
| ðŸ¤– Gemini Integration             | Prompts not caught by regex are sent to Gemini for LLM-based classification |
| ðŸ“Š Prompt Logging                 | All prompts and their classifications (Regex or Gemini) are logged to Google Sheets |
| âš ï¸ Review Mechanism              | Unknown or LLM-flagged threats are sent to a **Pending Review** list for human verification |
| ðŸ” Threat List Expansion (Optional) | Threat patterns can be updated manually or semi-automatically based on LLM feedback |

---

## ðŸ“‚ Project Structure

```
AI-Security-Prompt-Filter-n8n/
â”œâ”€â”€ README.md
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ workflow_diagram.png
â”‚   â””â”€â”€ PromptSafe.png
â”œâ”€â”€ workflows/
â”‚   â””â”€â”€ prompt_filter_workflow.json
â”œâ”€â”€ client/
â”‚   â””â”€â”€ PromptSafe.html
â”œâ”€â”€ sheets/
â”‚   â””â”€â”€ Sample_Google_sheets_structure.xlsx
â””â”€â”€ LICENSE
```
## Frontend Prompt Submission

The `client/PromptSafe.html` file is a simple HTML form to submit prompts to the n8n Webhook. It can be opened in a browser to test prompt classification locally.

![form preview](assets/PromptSafe.png)
---

## ðŸ§ª Sample Sheets

The Excel file `Sample_Google_sheets_structure.xlsx` contains:

1. **ThreatPatterns**  
   - List of dynamic regex patterns to detect malicious prompts.
2. **PromptLogs**  
   - Logs all prompts with classification (`Safe`, `Malicious`) and source (`Regex`, `Gemini`)
3. **PendingReview**  
   - Prompts flagged by Gemini but not caught by regex, for manual review and possible threat list expansion.

---

## ðŸš€ How It Works (Workflow Overview)

1. ðŸŒ User submits a prompt via Webhook
2. ðŸ“„ Google Sheet `ThreatPatterns` is fetched
3. ðŸ§  Code Node checks for regex matches
4. âš–ï¸ If matched â†’ labeled `Malicious (Regex)`  
   Else â†’ sent to Gemini API for classification
5. âœï¸ Results are logged in Google Sheets:
   - âœ… `PromptLogs` for all inputs
   - âš ï¸ `PendingReview` for LLM-flagged unknowns

---

## âš™ï¸ Setup

1. Clone this repo
2. Import the `.json` file into your n8n instance
3. Configure your:
   - Google Sheets credentials
   - Gemini API key
4. Start the workflow and test via form input

---

## ðŸ“Œ Use Case

This project was created to demonstrate AI security handling of adversarial or sensitive LLM inputs â€” useful for threat prompt classification, jailbreak detection, and real-world AI governance tooling.

---

## ðŸ”§ Tech Stack

- [x] **n8n** (self-hosted with persistent memory)
- [x] **Google Sheets** (for threat data + logging)
- [x] **Gemini API** (via HTTP Request Node)

---

## ðŸ§¾ License

This project is licensed under the [MIT License](https://opensource.org/licenses/MIT) â€” free to use, modify, and distribute with attribution.

---

## ðŸ“« Contact

> For suggestions, collaboration, or hiring queries, feel free to reach out via GitHub or LinkedIn!
